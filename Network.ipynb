{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ec612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import math, copy, time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Core encoder is a stack of N layers\"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Pass the input (and mask) through each layer in turn.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0965b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"Apply residual connection to any sublayer with the same size.\"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder is made up of self-attn and feed forward (defined below)\"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout=0.2):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Follow Figure 1 (left) for connections.\"\"\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Generic N layer decoder with masking.\"\"\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d933f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0dc9f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "\n",
    "    def __init__(self, src, trg=None):\n",
    "        self.src = src\n",
    "        self.src_mask = None\n",
    "        if trg is not None:\n",
    "            self.trg = trg\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask =Variable(\n",
    "            subsequent_mask(tgt.size(-1)))\n",
    "        tgt_mask=tgt_mask.to(device)\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture.\n",
    "    Base for this and many other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder,\n",
    "                 src_embed, tgt_embed, input_enc_len, input_dec_len, out_seq_len, d_model):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        # Encoder对象\n",
    "        self.decoder = decoder\n",
    "        # Decoder对象\n",
    "        self.src_embed = src_embed\n",
    "        # 源语言序列的编码，包括词嵌入和位置编码\n",
    "        self.tgt_embed = tgt_embed\n",
    "        #1 is the dimension\n",
    "        self.enc_input_fc = nn.Linear(1, d_model)\n",
    "        self.dec_input_fc = nn.Linear(1, d_model)\n",
    "        self.out_fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        # 目标语言序列的编码，包括词嵌入和位置编码\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        out=self.decode(self.encode(src, src_mask), src_mask,\n",
    "                           tgt, tgt_mask)\n",
    "        return out.to(device)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        in_enc = self.enc_input_fc(src)\n",
    "        in_enc=in_enc.to(device)\n",
    "        encode=self.encoder(self.src_embed(in_enc), src_mask)\n",
    "        return encode.to(device)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        in_dec = self.dec_input_fc(tgt)\n",
    "        result=self.decoder(self.tgt_embed(in_dec),\n",
    "                            memory, src_mask, tgt_mask)\n",
    "        out=self.out_fc(result)\n",
    "\n",
    "        return out.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_enc_len,input_dec_len, out_seq_len,N=6,\n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.2):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn),\n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(c(position)),\n",
    "        nn.Sequential(c(position)), input_enc_len, input_dec_len, out_seq_len,d_model)\n",
    "    model=model.to(device)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
