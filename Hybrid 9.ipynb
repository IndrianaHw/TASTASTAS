{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae3ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f401aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load & Preprocess Data ---\n",
    "# Example: Load QBO data with a 'Date' and '25 hPa' column\n",
    "data_df = pd.read_excel('QBO Data.xlsx')\n",
    "data_df['Date'] = pd.to_datetime(data_df['Date'], format='%d%m%Y')\n",
    "data_df.set_index('Date', inplace=True)\n",
    "series = data_df['25 hPa'].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e744f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Train-Test Split (90/10, sequential) ---\n",
    "train_size = int(len(series) * 0.9)\n",
    "train, test = series[:train_size], series[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb55d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Fit SARIMA on Training Data ---\n",
    "# (Tune your SARIMA parameters as needed)\n",
    "sarima = SARIMAX(train, order=(2,0,2), seasonal_order=(1,0,1,12))\n",
    "sarima_fit = sarima.fit(disp=False)\n",
    "sarima_pred_train = sarima_fit.fittedvalues\n",
    "sarima_pred_test = sarima_fit.forecast(steps=len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3adab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Compute Residuals for Training (actual - SARIMA) ---\n",
    "residual_train = train - sarima_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d8fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Feature Engineering for Transformer ---\n",
    "# Example: Use lag features (feel free to add more features)\n",
    "def create_lag_features(series, lags=[1,2,3]):\n",
    "    df = pd.DataFrame({'y': series})\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "    return df.dropna()\n",
    "\n",
    "lags = [1,2,3,12]  # Example lags\n",
    "full_df = create_lag_features(series, lags)\n",
    "full_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Align residuals with lagged features\n",
    "aligned_residual = residual_train[-len(full_df[:train_size-lags[-1]]):]  # For training\n",
    "aligned_sarima_pred_test = sarima_pred_test[-len(test)+lags[-1]:]       # For hybrid output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcdef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Prepare Data for Transformer (windowed input, residual target) ---\n",
    "# Split lag features into train/test\n",
    "X = full_df.drop('y', axis=1).values\n",
    "y = full_df['y'].values\n",
    "\n",
    "X_train = X[:train_size-lags[-1]]\n",
    "X_test = X[train_size-lags[-1]:]\n",
    "y_train = aligned_residual  # Target: residual SARIMA\n",
    "y_test = test[lags[-1]:]    # For evaluation\n",
    "\n",
    "# Scale features and residuals\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9fecd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Inputs have incompatible shapes. Received shapes (1, 4) and (1, 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m X_train_tr \u001b[38;5;241m=\u001b[39m X_train_scaled[:, np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[0;32m     21\u001b[0m X_test_tr \u001b[38;5;241m=\u001b[39m X_test_scaled[:, np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[1;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     26\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-3\u001b[39m),\n\u001b[0;32m     28\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     29\u001b[0m )\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mbuild_transformer\u001b[1;34m(input_shape, head_size, num_heads, ff_dim, num_blocks, mlp_units, dropout)\u001b[0m\n\u001b[0;32m      8\u001b[0m     x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mLayerNormalization(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)(x)\n\u001b[0;32m      9\u001b[0m     ff_output \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(ff_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mFlatten()(x)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m units \u001b[38;5;129;01min\u001b[39;00m mlp_units:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\merging\\base_merge.py:93\u001b[0m, in \u001b[0;36mMerge._compute_elemwise_op_output_shape\u001b[1;34m(self, shape1, shape2)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[1;32m---> 93\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs have incompatible shapes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m             )\n\u001b[0;32m     97\u001b[0m         output_shape\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(output_shape)\n",
      "\u001b[1;31mValueError\u001b[0m: Inputs have incompatible shapes. Received shapes (1, 4) and (1, 8)"
     ]
    }
   ],
   "source": [
    "# --- 7. Build Transformer Model (Simple Version) ---\n",
    "def build_transformer(input_shape, head_size=8, num_heads=2, ff_dim=8, num_blocks=2, mlp_units=[32], dropout=0.1):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    for _ in range(num_blocks):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(x, x)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        ff_output = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "        x = layers.Add()([x, ff_output])\n",
    "    x = layers.Flatten()(x)\n",
    "    for units in mlp_units:\n",
    "        x = layers.Dense(units, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "# Reshape for transformer: (samples, time steps, features)\n",
    "# Here each sample is a feature vector, not a sequence, so use (samples, 1, features)\n",
    "X_train_tr = X_train_scaled[:, np.newaxis, :]\n",
    "X_test_tr = X_test_scaled[:, np.newaxis, :]\n",
    "\n",
    "model = build_transformer(X_train_tr.shape[1:])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    metrics=[\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aefccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Train Transformer on Residuals ---\n",
    "checkpoint = ModelCheckpoint(\"best_hybrid_transformer.h5\", save_best_only=True, monitor='val_loss', mode='min')\n",
    "history = model.fit(\n",
    "    X_train_tr, y_train_scaled,\n",
    "    validation_split=0.1,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ca782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Predict Residuals on Test Data ---\n",
    "residual_pred_scaled = model.predict(X_test_tr)\n",
    "residual_pred = target_scaler.inverse_transform(residual_pred_scaled).flatten()\n",
    "\n",
    "# --- 10. Hybrid Prediction on Test: SARIMA + Transformer Residual ---\n",
    "hybrid_pred = aligned_sarima_pred_test + residual_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c24b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 11. Evaluation ---\n",
    "rmse_sarima = mean_squared_error(y_test, aligned_sarima_pred_test, squared=False)\n",
    "rmse_hybrid = mean_squared_error(y_test, hybrid_pred, squared=False)\n",
    "r2_sarima = r2_score(y_test, aligned_sarima_pred_test)\n",
    "r2_hybrid = r2_score(y_test, hybrid_pred)\n",
    "\n",
    "print(f\"SARIMA RMSE: {rmse_sarima:.3f}, Hybrid RMSE: {rmse_hybrid:.3f}\")\n",
    "print(f\"SARIMA R2: {r2_sarima:.3f}, Hybrid R2: {r2_hybrid:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d3f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 12. Plot Results ---\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(np.arange(len(series)), series, label=\"Actual\")\n",
    "plt.plot(np.arange(train_size, len(series)), aligned_sarima_pred_test, label=\"SARIMA\", color=\"green\")\n",
    "plt.plot(np.arange(train_size, len(series)), hybrid_pred, label=\"Hybrid\", color=\"red\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(\"Hybrid SARIMAâ€“Transformer Forecast\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"QBO (25 hPa)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
