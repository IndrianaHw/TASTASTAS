{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34d574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.dates as mdates\n",
    "import torch.nn.functional as F\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error,mean_squared_log_error\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d299a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_excel(\"QBO Data.xlsx\")\n",
    "columns_name = ['25 hPa']\n",
    "data_df = pd.DataFrame(Data, columns=columns_name)\n",
    "# time_series_numpy = Data.values.astype(float)  # Replace 'your_column_name' with the actual column name\n",
    "\n",
    "# Seasonal features (assuming you have a time index)\n",
    "Data['Date'] = pd.to_datetime(Data['Date'], format='%d%m%Y')\n",
    "Data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc2d330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((258, 12, 1), (258, 1), (65, 12, 1), (65, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data 80:20\n",
    "train, test = train_test_split(data_df, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Fit ARIMA(4, 0, 5)\n",
    "model = ARIMA(train, order=(4, 0, 5))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Ambil residual\n",
    "residuals = model_fit.resid\n",
    "\n",
    "# Normalisasi dan preprocessing residual untuk Transformer\n",
    "window_size = 12\n",
    "residual_series = residuals.values.reshape(-1, 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "residual_scaled = scaler.fit_transform(residual_series)\n",
    "\n",
    "def create_windowed_dataset(data_df, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_df) - window_size):\n",
    "        X.append(data_df[i:i + window_size])\n",
    "        y.append(data_df[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_windowed_dataset(residual_scaled, window_size)\n",
    "\n",
    "# Train-test split 80:20\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d728493",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RangeIndex' object has no attribute 'quarter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m add_rolling_statistics(data_df, window_sizes)\n\u001b[0;32m     78\u001b[0m add_interaction_features(data_df, window_sizes)\n\u001b[1;32m---> 79\u001b[0m \u001b[43madd_seasonal_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m add_rate_of_change(data_df, lags)\n\u001b[0;32m     81\u001b[0m add_cumulative_sum(data_df)\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36madd_seasonal_features\u001b[1;34m(data_df)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_seasonal_features\u001b[39m(data_df):\n\u001b[1;32m---> 28\u001b[0m     data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquarter\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquarter\u001b[49m\n\u001b[0;32m     29\u001b[0m     data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39myear\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RangeIndex' object has no attribute 'quarter'"
     ]
    }
   ],
   "source": [
    "def add_lag_features(data_df, lags):\n",
    "    for lag in lags:\n",
    "        column_name_lag = f'lag_{lag}'\n",
    "        data_df[column_name_lag] = data_df['25 hPa'].shift(lag)\n",
    "\n",
    "def add_rolling_statistics(data_df, window_sizes):\n",
    "    for window_size in window_sizes:\n",
    "        column_name_min = f'rolling_min_{window_size}'\n",
    "        column_name_max = f'rolling_max_{window_size}'\n",
    "        column_name_ma = f'moving_average_{window_size}'\n",
    "        column_name_ema = f'ema_{window_size}'\n",
    "        column_name_std = f'moving_std_{window_size}'\n",
    "        column_name_median = f'moving_median_{window_size}'\n",
    "\n",
    "        data_df[column_name_min] = data_df['25 hPa'].rolling(window=window_size).min()\n",
    "        data_df[column_name_max] = data_df['25 hPa'].rolling(window=window_size).max()\n",
    "        data_df[column_name_ma] = data_df['25 hPa'].rolling(window=window_size).mean()\n",
    "        data_df[column_name_ema] = data_df['25 hPa'].ewm(span=window_size, adjust=False).mean()\n",
    "        data_df[column_name_std] = data_df['25 hPa'].rolling(window=window_size).std()\n",
    "        data_df[column_name_median] = data_df['25 hPa'].rolling(window=window_size).median()\n",
    "\n",
    "def add_interaction_features(data_df, window_sizes):\n",
    "    for window_size in window_sizes:\n",
    "        column_name_interaction = f'interaction_ma_ema_{window_size}'\n",
    "        data_df[column_name_interaction] = data_df[f'moving_average_{window_size}'] * data_df[f'ema_{window_size}']\n",
    "\n",
    "def add_seasonal_features(data_df):\n",
    "    data_df['quarter'] = data_df.index.quarter\n",
    "    data_df['year'] = data_df.index.year\n",
    "\n",
    "def add_rate_of_change(data_df, lags):\n",
    "    for lag in lags:\n",
    "        column_name_roc = f'roc_{lag}'\n",
    "        data_df[column_name_roc] = (data_df['25 hPa'] - data_df['25 hPa'].shift(lag)) / data_df['25 hPa'].shift(lag)\n",
    "\n",
    "def add_cumulative_sum(data_df):\n",
    "    data_df['cumulative_sum'] = data_df['25 hPa'].cumsum()\n",
    "\n",
    "def add_ewma_std(data_df, window_sizes):\n",
    "    for window_size in window_sizes:\n",
    "        column_name_ewma_std = f'ewma_std_{window_size}'\n",
    "        data_df[column_name_ewma_std] = data_df['25 hPa'].ewm(span=window_size, adjust=False).std()\n",
    "\n",
    "def add_additional_statistics(data_df, window_sizes):\n",
    "    for window_size in window_sizes:\n",
    "        column_name_lagged_ma = f'lagged_ma_{window_size}'\n",
    "        column_name_lagged_ema = f'lagged_ema_{window_size}'\n",
    "        column_name_acf = f'autocorr_{window_size}'\n",
    "\n",
    "        data_df[column_name_lagged_ma] = data_df[f'moving_average_{window_size}'].shift(1)\n",
    "        data_df[column_name_lagged_ema] = data_df[f'ema_{window_size}'].shift(1)\n",
    "        data_df[column_name_acf] = data_df['25 hPa'].autocorr(lag=window_size)\n",
    "\n",
    "def add_z_scores(data_df):\n",
    "    # Z-Score based on standard deviation\n",
    "    data_df['z_score'] = (data_df['25 hPa'] - data_df['25 hPa'].mean()) / data_df['25 hPa'].std()\n",
    "\n",
    "    # Z-Score based on median and median absolute deviation (MAD)\n",
    "    median = data_df['25 hPa'].median()\n",
    "    mad = np.median(np.abs(data_df['25 hPa'] - median))\n",
    "    data_df['mad_z_score'] = (data_df['25 hPa'] - median) / mad\n",
    "\n",
    "def add_log_returns(data_df):\n",
    "    data_df['log_returns'] = np.log(data_df['25 hPa'] / data_df['25 hPa'].shift(1))\n",
    "\n",
    "\n",
    "# List of lags for lag features\n",
    "lags = [1, 2, 3]\n",
    "# Lags represent the time intervals used to shift and create lag features, capturing historical values at different points in the past.\n",
    "\n",
    "# List of window sizes for rolling statistics and other features\n",
    "window_sizes = [2, 3, 6, 9, 12, 24]\n",
    "# Window sizes determine the size of the rolling windows used for calculating statistics and features. Different sizes capture varying trends and patterns over specified periods.\n",
    "\n",
    "\n",
    "add_lag_features(data_df, lags)\n",
    "add_rolling_statistics(data_df, window_sizes)\n",
    "add_interaction_features(data_df, window_sizes)\n",
    "add_seasonal_features(data_df)\n",
    "add_rate_of_change(data_df, lags)\n",
    "add_cumulative_sum(data_df)\n",
    "add_ewma_std(data_df, window_sizes)\n",
    "add_additional_statistics(data_df, window_sizes)\n",
    "add_z_scores(data_df)\n",
    "add_log_returns(data_df)\n",
    "\n",
    "data_df_drop = data_df.dropna()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
